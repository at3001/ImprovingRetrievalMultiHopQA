{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0JPs77nUVFw"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVpbBtqaUVm0",
        "outputId": "ce97b5a7-9a2c-44d9-c549-ef06e6eaf953"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llama_index\n",
            "  Downloading llama_index-0.10.34-py3-none-any.whl (6.9 kB)\n",
            "Collecting llama-index-agent-openai<0.3.0,>=0.1.4 (from llama_index)\n",
            "  Downloading llama_index_agent_openai-0.2.3-py3-none-any.whl (13 kB)\n",
            "Collecting llama-index-cli<0.2.0,>=0.1.2 (from llama_index)\n",
            "  Downloading llama_index_cli-0.1.12-py3-none-any.whl (26 kB)\n",
            "Collecting llama-index-core<0.11.0,>=0.10.34 (from llama_index)\n",
            "  Downloading llama_index_core-0.10.34-py3-none-any.whl (15.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.4/15.4 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llama-index-embeddings-openai<0.2.0,>=0.1.5 (from llama_index)\n",
            "  Downloading llama_index_embeddings_openai-0.1.9-py3-none-any.whl (6.0 kB)\n",
            "Collecting llama-index-indices-managed-llama-cloud<0.2.0,>=0.1.2 (from llama_index)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.1.6-py3-none-any.whl (6.7 kB)\n",
            "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama_index)\n",
            "  Downloading llama_index_legacy-0.9.48-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llama-index-llms-openai<0.2.0,>=0.1.13 (from llama_index)\n",
            "  Downloading llama_index_llms_openai-0.1.16-py3-none-any.whl (10 kB)\n",
            "Collecting llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 (from llama_index)\n",
            "  Downloading llama_index_multi_modal_llms_openai-0.1.5-py3-none-any.whl (5.8 kB)\n",
            "Collecting llama-index-program-openai<0.2.0,>=0.1.3 (from llama_index)\n",
            "  Downloading llama_index_program_openai-0.1.6-py3-none-any.whl (5.2 kB)\n",
            "Collecting llama-index-question-gen-openai<0.2.0,>=0.1.2 (from llama_index)\n",
            "  Downloading llama_index_question_gen_openai-0.1.3-py3-none-any.whl (2.9 kB)\n",
            "Collecting llama-index-readers-file<0.2.0,>=0.1.4 (from llama_index)\n",
            "  Downloading llama_index_readers_file-0.1.20-py3-none-any.whl (37 kB)\n",
            "Collecting llama-index-readers-llama-parse<0.2.0,>=0.1.2 (from llama_index)\n",
            "  Downloading llama_index_readers_llama_parse-0.1.4-py3-none-any.whl (2.5 kB)\n",
            "Collecting openai>=1.14.0 (from llama-index-agent-openai<0.3.0,>=0.1.4->llama_index)\n",
            "  Downloading openai-1.25.1-py3-none-any.whl (312 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m312.9/312.9 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.34->llama_index) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.34->llama_index) (2.0.29)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.34->llama_index) (3.9.5)\n",
            "Collecting dataclasses-json (from llama-index-core<0.11.0,>=0.10.34->llama_index)\n",
            "  Downloading dataclasses_json-0.6.5-py3-none-any.whl (28 kB)\n",
            "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.11.0,>=0.10.34->llama_index)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.11.0,>=0.10.34->llama_index)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.34->llama_index) (2023.6.0)\n",
            "Collecting httpx (from llama-index-core<0.11.0,>=0.10.34->llama_index)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llamaindex-py-client<0.2.0,>=0.1.18 (from llama-index-core<0.11.0,>=0.10.34->llama_index)\n",
            "  Downloading llamaindex_py_client-0.1.19-py3-none-any.whl (141 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.34->llama_index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.34->llama_index) (3.3)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.34->llama_index) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.34->llama_index) (1.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.34->llama_index) (2.0.3)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.34->llama_index) (9.4.0)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.34->llama_index) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.34->llama_index) (8.2.3)\n",
            "Collecting tiktoken>=0.3.3 (from llama-index-core<0.11.0,>=0.10.34->llama_index)\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m75.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.34->llama_index) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.34->llama_index) (4.11.0)\n",
            "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.11.0,>=0.10.34->llama_index)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.34->llama_index) (1.14.1)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama_index) (4.12.3)\n",
            "Collecting pypdf<5.0.0,>=4.0.1 (from llama-index-readers-file<0.2.0,>=0.1.4->llama_index)\n",
            "  Downloading pypdf-4.2.0-py3-none-any.whl (290 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.2.0,>=0.1.4->llama_index)\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Collecting llama-parse<0.5.0,>=0.4.0 (from llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama_index)\n",
            "  Downloading llama_parse-0.4.2-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.34->llama_index) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.34->llama_index) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.34->llama_index) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.34->llama_index) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.34->llama_index) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.34->llama_index) (4.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama_index) (2.5)\n",
            "Requirement already satisfied: pydantic>=1.10 in /usr/local/lib/python3.10/dist-packages (from llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.34->llama_index) (2.7.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.34->llama_index) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.34->llama_index) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx->llama-index-core<0.11.0,>=0.10.34->llama_index)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.34->llama_index) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.34->llama_index) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.34->llama_index)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.34->llama_index) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.34->llama_index) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.34->llama_index) (2023.12.25)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.3.0,>=0.1.4->llama_index) (1.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.34->llama_index) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.34->llama_index) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.34->llama_index) (3.0.3)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.34->llama_index)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.11.0,>=0.10.34->llama_index)\n",
            "  Downloading marshmallow-3.21.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.34->llama_index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.34->llama_index) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.34->llama_index) (2024.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.11.0,>=0.10.34->llama_index) (1.2.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.34->llama_index) (24.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.34->llama_index) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.34->llama_index) (2.18.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.34->llama_index) (1.16.0)\n",
            "Installing collected packages: striprtf, dirtyjson, pypdf, mypy-extensions, marshmallow, h11, deprecated, typing-inspect, tiktoken, httpcore, httpx, dataclasses-json, openai, llamaindex-py-client, llama-index-legacy, llama-index-core, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-program-openai, llama-index-question-gen-openai, llama_index\n",
            "Successfully installed dataclasses-json-0.6.5 deprecated-1.2.14 dirtyjson-1.0.8 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 llama-index-agent-openai-0.2.3 llama-index-cli-0.1.12 llama-index-core-0.10.34 llama-index-embeddings-openai-0.1.9 llama-index-indices-managed-llama-cloud-0.1.6 llama-index-legacy-0.9.48 llama-index-llms-openai-0.1.16 llama-index-multi-modal-llms-openai-0.1.5 llama-index-program-openai-0.1.6 llama-index-question-gen-openai-0.1.3 llama-index-readers-file-0.1.20 llama-index-readers-llama-parse-0.1.4 llama-parse-0.4.2 llama_index-0.10.34 llamaindex-py-client-0.1.19 marshmallow-3.21.2 mypy-extensions-1.0.0 openai-1.25.1 pypdf-4.2.0 striprtf-0.0.26 tiktoken-0.6.0 typing-inspect-0.9.0\n",
            "Collecting llama-index-embeddings-huggingface\n",
            "  Downloading llama_index_embeddings_huggingface-0.2.0-py3-none-any.whl (7.1 kB)\n",
            "Requirement already satisfied: huggingface-hub[inference]>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-embeddings-huggingface) (0.23.0)\n",
            "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-embeddings-huggingface) (0.10.34)\n",
            "Collecting sentence-transformers<3.0.0,>=2.6.1 (from llama-index-embeddings-huggingface)\n",
            "  Downloading sentence_transformers-2.7.0-py3-none-any.whl (171 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (4.11.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.9.5)\n",
            "Collecting minijinja>=1.0 (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface)\n",
            "  Downloading minijinja-2.0.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (853 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m853.2/853.2 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2.0.29)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (0.6.5)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.0.8)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (0.27.0)\n",
            "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.18 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (0.1.19)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (3.3)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.25.2)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.25.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2.0.3)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (9.4.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (8.2.3)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (0.6.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.14.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface) (4.40.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface) (2.2.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface) (1.11.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (4.0.3)\n",
            "Requirement already satisfied: pydantic>=1.10 in /usr/local/lib/python3.10/dist-packages (from llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2.7.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (0.14.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2023.12.25)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (3.0.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface) (1.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface) (3.1.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.11.0->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.11.0->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.11.0->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.11.0->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11.0->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.11.0->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.11.0->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface) (0.4.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (3.21.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2024.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface) (3.5.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.2.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2.18.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, minijinja, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, sentence-transformers, llama-index-embeddings-huggingface\n",
            "Successfully installed llama-index-embeddings-huggingface-0.2.0 minijinja-2.0.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 sentence-transformers-2.7.0\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.25.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.2)\n",
            "Collecting llama-index-postprocessor-cohere-rerank\n",
            "  Downloading llama_index_postprocessor_cohere_rerank-0.1.4-py3-none-any.whl (2.7 kB)\n",
            "Collecting cohere<6.0.0,>=5.1.1 (from llama-index-postprocessor-cohere-rerank)\n",
            "  Downloading cohere-5.3.4-py3-none-any.whl (151 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.2/151.2 kB\u001b[0m \u001b[31m278.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: llama-index-core<0.11.0,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-postprocessor-cohere-rerank) (0.10.34)\n",
            "Collecting fastavro<2.0.0,>=1.9.4 (from cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank)\n",
            "  Downloading fastavro-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: httpx>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank) (0.27.0)\n",
            "Collecting httpx-sse<0.5.0,>=0.4.0 (from cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: pydantic>=1.9.2 in /usr/local/lib/python3.10/dist-packages (from cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank) (2.7.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank) (0.19.1)\n",
            "Collecting types-requests<3.0.0,>=2.0.0 (from cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank)\n",
            "  Downloading types_requests-2.31.0.20240406-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank) (4.11.0)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-postprocessor-cohere-rerank) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-postprocessor-cohere-rerank) (2.0.29)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-postprocessor-cohere-rerank) (3.9.5)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-postprocessor-cohere-rerank) (0.6.5)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-postprocessor-cohere-rerank) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-postprocessor-cohere-rerank) (1.0.8)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-postprocessor-cohere-rerank) (2023.6.0)\n",
            "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.18 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-postprocessor-cohere-rerank) (0.1.19)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-postprocessor-cohere-rerank) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-postprocessor-cohere-rerank) (3.3)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-postprocessor-cohere-rerank) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-postprocessor-cohere-rerank) (1.25.2)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-postprocessor-cohere-rerank) (1.25.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-postprocessor-cohere-rerank) (2.0.3)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-postprocessor-cohere-rerank) (9.4.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-postprocessor-cohere-rerank) (8.2.3)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-postprocessor-cohere-rerank) (0.6.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-postprocessor-cohere-rerank) (4.66.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-postprocessor-cohere-rerank) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-postprocessor-cohere-rerank) (1.14.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-postprocessor-cohere-rerank) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-postprocessor-cohere-rerank) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-postprocessor-cohere-rerank) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-postprocessor-cohere-rerank) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-postprocessor-cohere-rerank) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-postprocessor-cohere-rerank) (4.0.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.21.2->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank) (0.14.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-postprocessor-cohere-rerank) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-postprocessor-cohere-rerank) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-postprocessor-cohere-rerank) (2023.12.25)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-postprocessor-cohere-rerank) (1.7.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.2->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.2->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank) (2.18.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-postprocessor-cohere-rerank) (3.0.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers<0.20,>=0.19->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank) (0.23.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.1->llama-index-postprocessor-cohere-rerank) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-postprocessor-cohere-rerank) (3.21.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-postprocessor-cohere-rerank) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-postprocessor-cohere-rerank) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-postprocessor-cohere-rerank) (2024.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.21.2->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank) (1.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<0.20,>=0.19->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank) (3.14.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<0.20,>=0.19->cohere<6.0.0,>=5.1.1->llama-index-postprocessor-cohere-rerank) (24.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-postprocessor-cohere-rerank) (1.16.0)\n",
            "Installing collected packages: types-requests, httpx-sse, fastavro, cohere, llama-index-postprocessor-cohere-rerank\n",
            "Successfully installed cohere-5.3.4 fastavro-1.9.4 httpx-sse-0.4.0 llama-index-postprocessor-cohere-rerank-0.1.4 types-requests-2.31.0.20240406\n"
          ]
        }
      ],
      "source": [
        "!pip install -q datasets\n",
        "!pip install llama_index\n",
        "!pip install llama-index-embeddings-huggingface\n",
        "!pip install openai\n",
        "!pip install llama-index-postprocessor-cohere-rerank\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LI3WBnylUWX_"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YJulilR373jZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14-Haw8eUWZw"
      },
      "outputs": [],
      "source": [
        "\n",
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.core.schema import TextNode\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core.vector_stores import VectorStoreQuery\n",
        "from llama_index.llms.openai import OpenAI as oai\n",
        "\n",
        "\n",
        "from llama_index.postprocessor.cohere_rerank import CohereRerank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enFoK-TPUWdT"
      },
      "outputs": [],
      "source": [
        "# OS settings for API keys\n",
        "os.environ[\"OPENAI_API_KEY\"] = ##\n",
        "os.environ[\"PINECONE_API_KEY\"] = ##\n",
        "os.environ[\"PINECONE_ENV\"] = ##\n",
        "os.environ[\"COHERE_API_KEY\"] = ##"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import openai\n",
        "from openai import OpenAI"
      ],
      "metadata": {
        "id": "LDTTGV1I4kcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVKabKE2UWjB"
      },
      "source": [
        "## Experiment with hotpot QA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYWbdAzwUYYm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304,
          "referenced_widgets": [
            "bd8acfc00a364309819f1b9ee383f5eb",
            "c749d8363dd543cdb054774ee99d86bc",
            "6e03d1c36df54b1696d2acf93506bb27",
            "186f4b826e0f41e79d5662725e4598d9",
            "c54731489f0944ff90e68ace218b949f",
            "a40181f93e9a43b99b884ee37a64964e",
            "1ecdbc8a49a443beac00462cc951d8a4",
            "7d12032a287c4e04bbce3eda6a4071f6",
            "56d364085db14ea2a30d3cedb02d19d3",
            "6f53cbd2428f40678cc9f7416d82b29d",
            "3f8e69f8fbe24fa0bab6999f8b3ed18e",
            "d5aa3083f08945fdb20e5d149daa0e06",
            "e6d5b207a9724c70bdbb81ac9f7d33a7",
            "7e43f482a533400c86711f2e789add89",
            "d69ee5c57f4d4d51b8b674b436849d5d",
            "08a30059c3734c33aa0b53f63f02bf39",
            "f509c7c745204db1a9d46177311ce67e",
            "de2f096f6e8c48609cf8caef3752f7b7",
            "fb917b0891774050b8c788ec08a24712",
            "605e1d3c5c1a44efb2221f2ed609f5dc",
            "d581930dbe1844cba69989ef3eaebf0d",
            "0cc8105429f948c3ba0695a6264c26d5",
            "9c2ea52a42f94179b51ab3e75324faac",
            "f1e2db5305a64ca2a91397670adcc7d9",
            "0b0cd9d0db604907a739690bf41aeede",
            "60203443bc3d4eeca6267f91143235b9",
            "c9a25e6d20f940e29b5dc2fed9452527",
            "09b17a3214f1407b81543e63576af05e",
            "e70a6d7cbce343fe8399ea5da51bf5c2",
            "92b8ad64e3cf415c9d509bbcf611e6e3",
            "328c637233c545a1bbe7e30c4e77627d",
            "52944e9d3d2e4bd3bab02f87c9572fad",
            "a8151b84286d4614b064745aaf6e6e63",
            "ca8b22e2ca88404492daa9c6d9b79b36",
            "9ca8fdc025ef49dc828b84ee2562ec81",
            "c97c6a7b8a164eec97ad565b8e681728",
            "1150ac74e6d944eaa5e42858271e066d",
            "881c8117ef81444ba53d7194f7ddd047",
            "eb63fbf9a71348b08682b6d37d3e89a9",
            "7417376e2a954792a045d9447eb87799",
            "ea96c41e05bb487091e4984454e48989",
            "124b9d76a46440dbb134cc70ba560e0d",
            "76d1baa0125947349d04ac0cb3024163",
            "8a7ccd159d6c482584e9386b17d3e58a",
            "3783e00fb4084d0ebabf53a6fc34714e",
            "3a7852d5421b4a2184e06c6d30f64d57",
            "08cdcd40f3f54277833899c3cf2ecaee",
            "2d43293bd0904fdaa606ca9e3b81e391",
            "b522c2adf61a4e318665fe2c25117d58",
            "c8af285305f84d3d8f04fb7324f2ee43",
            "e8ee252696b1408f83dc78e2218b2674",
            "0fc736e1f6744eb5a818628a2e2e6481",
            "4183eb71ab05490ea949f7f350def58e",
            "23307b0a6e1e42d0b55bc9bd066d2f24",
            "99ca4bc670b84bc5b4ec952ce2c3eae9"
          ]
        },
        "outputId": "69524182-c2cf-4758-dd06-426b189f75e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/301M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bd8acfc00a364309819f1b9ee383f5eb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/31.1M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d5aa3083f08945fdb20e5d149daa0e06"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/27.5M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9c2ea52a42f94179b51ab3e75324faac"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/90447 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ca8b22e2ca88404492daa9c6d9b79b36"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/7405 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3783e00fb4084d0ebabf53a6fc34714e"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"hotpot_qa\", \"distractor\", split = \"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-BtgW0AWeAY",
        "outputId": "dc6ca93f-4131-4737-c0ee-8601e78f7252"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['id', 'question', 'answer', 'type', 'level', 'supporting_facts', 'context'],\n",
              "    num_rows: 90447\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZ7ECMx8XJTE"
      },
      "outputs": [],
      "source": [
        "def convert_list_to_sent(para_list):\n",
        "  \"\"\"\n",
        "  Para list is a list of list of sentences\n",
        "  \"\"\"\n",
        "\n",
        "  return [' '.join(x) for x in para_list]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract all contexts that will be used to build the index for retrieval"
      ],
      "metadata": {
        "id": "AIGwcyanvNOl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oy_xNwOKXJU9",
        "outputId": "d5f8f028-f58a-446c-a11a-ab76b3bda0ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3000/3000 [00:00<00:00, 4337.28it/s]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "contexts = []\n",
        "for i in tqdm(range(3000)):\n",
        "  contexts += dataset[i]['context']['sentences']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBUTctIrq_-a",
        "outputId": "88bb6e65-d977-485e-b8a2-220f7d7d4e34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "William Jefferson White (December 25, 1831 – April 17, 1913) was a civil rights leader, minister, educator, and journalist in Augusta, Georgia.  He was the founder of Harmony Baptist Church in Augusta in 1869 as well as other churches.  He also was a co-founder of the Augusta Institute in 1867, which would become Morehouse College.  He also helped found Atlanta University and was a trustee of both schools.  He was a founder in 1880 and the managing editor of the \"Georgia Baptist\", a leading African American newspaper for many years.  He was an outspoken civil rights leader.\n"
          ]
        }
      ],
      "source": [
        "contexts = [tuple(context) for context in contexts]\n",
        "contexts = list(set(contexts))\n",
        "contexts = [list(context) for context in contexts]\n",
        "contexts = convert_list_to_sent(contexts)\n",
        "print(contexts[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(contexts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4K3TEKRJrHr",
        "outputId": "1dd35594-c2c1-4c70-9450-9acde51aeaef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28356"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvHFXEeFv7r5",
        "outputId": "3af289ef-316f-4b56-b5af-faed02488568"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "28356\n"
          ]
        }
      ],
      "source": [
        "print(len(contexts))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33LoRbBFro96"
      },
      "source": [
        "## Lets construct some data\n",
        "\n",
        "While we get multiple contexts, we test on 100. The idea being retrieval across a variety of vector DB noise. 100 further makes it possible to do manual analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjPbU2Muquaj",
        "outputId": "2b85b5b2-f94a-4e0a-862f-35e9b7de63a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 1331.41it/s]\n"
          ]
        }
      ],
      "source": [
        "questions = [dataset[i]['question'] for i in range(100)]\n",
        "supporting_titles = [dataset[i]['supporting_facts']['title'] for i in range(100)]\n",
        "answers = [dataset[i]['answer'] for i in range(100)]\n",
        "level = [dataset[i]['level'] for i in range(100)]\n",
        "\n",
        "supporting_articles = []\n",
        "for i in tqdm(range(100)):\n",
        "  sup_titles = list(set(supporting_titles[i]))\n",
        "  sup_1 = sup_titles[0]\n",
        "  sup_2 = sup_titles[1]\n",
        "\n",
        "  idx_1 = dataset[i]['context']['title'].index(sup_1)\n",
        "  idx_2 = dataset[i]['context']['title'].index(sup_2)\n",
        "\n",
        "  supporting_articles.append([' '.join(dataset[i]['context']['sentences'][idx_1]), \\\n",
        "                              ' '.join(dataset[i]['context']['sentences'][idx_2])])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "questions[9]\n",
        "\n",
        "supporting_articles[9]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGmjgucc5JwK",
        "outputId": "403d99a6-cd01-4a0b-af41-ac7538994ffd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Mount Panorama Circuit is a motor racing track located in Bathurst, New South Wales, Australia.  It is situated on a hill with the dual official names of Mount Panorama and Wahluu and is best known as the home of the Bathurst 1000 motor race held each October, and the Bathurst 12 Hour event held each February.  The 6.213 km long track is technically a street circuit, and is a public road, with normal speed restrictions, when no racing events are being run, and there are many residences which can only be accessed from the circuit.',\n",
              " 'The 2013 Liqui Moly Bathurst 12 Hour was an endurance race for a variety of GT and touring car classes, including: GT3 cars, GT4 cars, Group 3E Series Production Cars and Dubai 24 Hour cars.  The event, which was staged at the Mount Panorama Circuit, near Bathurst, in New South Wales, Australia on 10 February 2013, was the eleventh running of the Bathurst 12 Hour.  The race also incorporated the opening round of the 2013 Australian GT Championship.  The Australian GT Championship was to compete as the first hour only and cars were permitted to enter for only that hour or to cross-enter for both the first hour and continue for the endurance race.']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMsH5W56spXx"
      },
      "source": [
        "## Setting up retrieval\n",
        "\n",
        "\n",
        "Here, we create the indexa and test it out to see how it works\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSIL1j5s8dm2"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.core import Document\n",
        "\n",
        "documents = [Document(text=t) for t in contexts]\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents,\n",
        "    llm = oai(temperature = 0, model = 'gpt-3.5-turbo')\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "GFzb62qd8m_m",
        "outputId": "d0d899c8-ab97-4540-fd77-9eb0a44b134e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'In which American football game was Malcolm Smith named Most Valuable player?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "questions[13]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWNOEwYqr2GL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a095b79-0562-4085-cf2f-cf843d55c611"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The length of the Mount Panorama Circuit, where the 2013 Liqui Moly Bathurst 12 Hour was staged, is approximately 6.213 kilometers.\n"
          ]
        }
      ],
      "source": [
        "#Example of generation using one question\n",
        "query_str = \"What is the length of the Mount Panorama Circuit, where the 2013 Liqui Moly Bathurst 12 Hour was staged?\"\n",
        "q = index.as_query_engine(similarity_top_k = 4)\n",
        "query_result = q.query(query_str)\n",
        "\n",
        "print(query_result.response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answers[13]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "GYo3vQgQ1Js9",
        "outputId": "3b0f0a50-de9a-40a0-a491-99a24c79f62d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Super Bowl XLVIII'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(query_result.source_nodes[0].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8pkI4Fw0RVi",
        "outputId": "69655ea7-9f0e-4d5a-a926-492c6e175cff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The 2017 Liqui Moly Bathurst 12 Hour endurance race for GT and touring car classes, GT3 and GT4 cars was staged on the Mount Panorama Circuit, near Bathurst, in New South Wales, Australia 5 February 2017.  The 15th running of the Bathurst 12 Hour constituted the opening round of the 2017 Intercontinental GT Challenge Series.  For the first time, the winners of the race were awarded the Australian Tourist Trophy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKJ8J5YHr2H2",
        "outputId": "96157f01-2020-4ef5-fc73-ad3a3d4813ff"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Longs Drugs is an American chain with approximately 40 drug stores throughout the state of Hawaii.',\n",
              " \"Warren Bryant was the CEO of Longs Drugs Store Corporation out of California prior to the retail chain's acquisition by CVS/Caremark.  Hired in 2002 to Longs, he was Senior Vice President of The Kroger Company. , a retail grocery chain, from 1999 to 2002.  Prior to that, from 1996 to 1999, he was President and Chief Executive Officer of Dillon Companies, Inc., a retail grocery chain and subsidiary of The Kroger Co.  He is also a director of OfficeMax Incorporated.\"]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "supporting_articles[20]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "questions[21]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "w_qXx1eEUuED",
        "outputId": "5374c440-493a-4dd6-d8bc-bdd8997f1559"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Which  American politician did Donahue replaced '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nXPf_DFkdXvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vanilla RAG with HotpotQA"
      ],
      "metadata": {
        "id": "KO34wBz150ug"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We run this for 100 examples and also compute the needed metrics"
      ],
      "metadata": {
        "id": "EddhtHnrvaBN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uImT9KLgr2LR",
        "outputId": "7ece32a7-13d0-4a09-8b22-8d957a779037"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [04:57<00:00,  2.97s/it]\n"
          ]
        }
      ],
      "source": [
        "#Evaluating hits @ k [in our case k here is 5]\n",
        "\n",
        "similarity_top_k = 4\n",
        "hit_per_q = []\n",
        "num_hit = 0\n",
        "num_hit_rel = 0\n",
        "responses = []\n",
        "mrr_sum = 0\n",
        "retrieved_docs = []\n",
        "\n",
        "engine = index.as_query_engine(similarity_top_k = similarity_top_k)\n",
        "\n",
        "for i in tqdm(range(0, 100)):\n",
        "  hit_perc = 0\n",
        "  q = questions[i]\n",
        "  hit_flag = False\n",
        "\n",
        "  #Get answer\n",
        "  query_result = engine.query(q)\n",
        "\n",
        "  #Get supporting articles\n",
        "  hits = [query_result.source_nodes[i].text for i in range(len(query_result.source_nodes))]\n",
        "\n",
        "  responses.append(query_result.response)\n",
        "  retrieved_docs.append(hits)\n",
        "\n",
        "  #Compute metrics\n",
        "  for j in range(len(supporting_articles[i])):\n",
        "    if supporting_articles[i][j] in hits:\n",
        "      hit_flag = True\n",
        "      num_hit += 1\n",
        "      hit_perc += 1\n",
        "\n",
        "  for j in range(len(hits)):\n",
        "    if hits[j] in supporting_articles[i]:\n",
        "      mrr_sum += (1/(j+1))\n",
        "      break\n",
        "\n",
        "\n",
        "  if hit_flag:\n",
        "    num_hit_rel += 1\n",
        "  hit_per_q += [hit_perc]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute some stats\n",
        "print(sum([hit_per_q[i] == 0 for i in range(100)]))\n",
        "print(sum([hit_per_q[i] == 1 for i in range(100)]))\n",
        "print(sum([hit_per_q[i] == 2 for i in range(100)]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_vDLm9i2o__",
        "outputId": "202551e5-576b-40cb-9628-8fed9b25e244"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "39\n",
            "58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Computing further stats on retrieval\n",
        "\n",
        "hard_hits = 0\n",
        "num_hard = 0\n",
        "for i in range(100):\n",
        "  if level[i] == \"hard\":\n",
        "    num_hard += 1\n",
        "    if hit_per_q[i] == 2:\n",
        "      hard_hits += 1\n",
        "\n",
        "print(num_hard)\n",
        "print(hard_hits / num_hard)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZ0BBDRX5cee",
        "outputId": "97a96b52-e1fb-4018-b5c0-87df20e8b146"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16\n",
            "0.625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(num_hit_rel / 100)\n",
        "print(num_hit / 200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVx5znlX6g_X",
        "outputId": "f8546263-de31-4995-eb99-421661985507"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.97\n",
            "0.775\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating generation performance\n",
        "\n",
        "For this, we will make use of an evaluation query run on GPT 3.5 Turbo to see if the responses match\n",
        "\n",
        "Note this is only the first step. **For the obtained results, we validate each of them manually on a spreadsheet**"
      ],
      "metadata": {
        "id": "TafSx69u7lNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "import openai"
      ],
      "metadata": {
        "id": "G5cE1jTSC4Ha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI(api_key = os.environ['OPENAI_API_KEY'])\n"
      ],
      "metadata": {
        "id": "cFbZMlziC8uN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_prompt_check(idx):\n",
        "\n",
        "  prompt = \"For the question: \" + questions[idx] + \" Are the following two answers both saying the same thing? \" + \\\n",
        "    \"Answer 1: \" + responses[idx] + \", Answer 2: \" + \\\n",
        "    answers[idx] + \". Respond with yes or no\"\n",
        "\n",
        "  return prompt\n",
        "\n",
        "\n",
        "ans_cor = []\n",
        "\n",
        "#We run this for all answers as a first round of checks\n",
        "for i in tqdm(range(100)):\n",
        "  prompt = create_prompt_check(i)\n",
        "\n",
        "\n",
        "\n",
        "  response = client.completions.create(\n",
        "    model=\"gpt-3.5-turbo-instruct\",\n",
        "    prompt=prompt,\n",
        "    temperature = 0\n",
        "  )\n",
        "\n",
        "\n",
        "  if response.choices[0].text.strip().lower().startswith(\"yes\"):\n",
        "    ans_cor.append(1)\n",
        "  else:\n",
        "    ans_cor.append(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lf1rZztP7nrh",
        "outputId": "94b75740-f0d5-49a9-c3e5-59c9fb0cb51c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:23<00:00,  4.18it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is accuracy based on this. We manually evaluate all of the checks based on the CSV extracted below. This process is just to get a good V1"
      ],
      "metadata": {
        "id": "9vHS9Byxvvrw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#accuracy of the answers\n",
        "\n",
        "np.sum(ans_cor) / len(ans_cor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nu5kfOdx7ntp",
        "outputId": "06d8b973-e0cf-4469-b0d9-89120be109af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.68"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Exporting this evaluation as a dataframe\n",
        "\n",
        "eval_df = pd.DataFrame({\n",
        "    'question': questions,\n",
        "    'ground_truth': answers,\n",
        "    'rag_response': responses,\n",
        "    'level': level,\n",
        "    'correct_or_not': ans_cor\n",
        "})\n",
        "\n",
        "eval_df.to_csv(\"vanilla_hotpot_eval_k4.csv\")"
      ],
      "metadata": {
        "id": "SgTDgKH1EoXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Aside: Baseline accuracy evaluation on first 100 based on GPT 3.5\n",
        "\n",
        "**This is to see accuracy when GPT 3.5 is provided ground truth passages. This basically helps us verify whether retrieval is an issue or it is generation. If accuracy here is very high, it just means we need to retrieve better as given the correct contexts, GPT 3.5 can do a pretty good job with the task.**"
      ],
      "metadata": {
        "id": "E-uIyzleKRTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_rag_prompt(passage1, passage2, question):\n",
        "\n",
        "  prompt = \"\"\"\n",
        "    We have provided context information below.\n",
        "\n",
        "    ---------------------\n",
        "    {passage1}\n",
        "    ---------------------\n",
        "    {passage2}\n",
        "    ---------------------\n",
        "    Given this information, please answer the question: {question}\n",
        "  \"\"\".format(passage1 = passage1, passage2 = passage2, question = question)\n",
        "\n",
        "  return prompt\n",
        "\n",
        "responses = []\n",
        "\n",
        "#We run RAG evaluation with ground truth contexts\n",
        "for i in tqdm(range(len(questions))):\n",
        "  passage1 = supporting_articles[i][0]\n",
        "  passage2 = supporting_articles[i][1]\n",
        "  question = questions[i]\n",
        "\n",
        "\n",
        "  prompt = create_rag_prompt(passage1, passage2, question)\n",
        "\n",
        "  response = client.completions.create(\n",
        "    model=\"gpt-3.5-turbo-instruct\",\n",
        "    prompt=prompt,\n",
        "    temperature = 0,\n",
        "    max_tokens = 150\n",
        "  )\n",
        "\n",
        "  responses.append(response.choices[0].text.strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f95-dH6A6AKr",
        "outputId": "62840dbc-69ea-4c45-a05e-e8314d10474b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:59<00:00,  1.67it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#We run this check and eventually use manual analysis as in the before case\n",
        "ans_cor = []\n",
        "\n",
        "for i in tqdm(range(len(questions))):\n",
        "  prompt = create_prompt_check(i)\n",
        "\n",
        "  response = client.completions.create(\n",
        "    model=\"gpt-3.5-turbo-instruct\",\n",
        "    prompt=prompt,\n",
        "    temperature = 0\n",
        "  )\n",
        "\n",
        "\n",
        "  if response.choices[0].text.strip().lower().startswith(\"yes\"):\n",
        "    ans_cor.append(1)\n",
        "  else:\n",
        "    ans_cor.append(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrLf2YqX6AM_",
        "outputId": "0c2c5912-5620-4f3a-f7b8-c14fab31db96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:36<00:00,  2.76it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#accuracy of the answers\n",
        "\n",
        "np.sum(ans_cor) / len(ans_cor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7907beff-f79b-4685-f0c6-003d6aa99ed0",
        "id": "7EAzOPyD7Txx"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.83"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Exporting this evaluation as a dataframe\n",
        "\n",
        "eval_df = pd.DataFrame({\n",
        "    'question': questions,\n",
        "    'ground_truth': answers,\n",
        "    'rag_response': responses,\n",
        "    'level': level,\n",
        "    'correct_or_not': ans_cor\n",
        "})\n",
        "\n",
        "eval_df.to_csv(\"groundtruth_hotpot_eval.csv\")"
      ],
      "metadata": {
        "id": "G1kw6WZG7TyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P3gMvrX86AR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKwQTpv0XJY4"
      },
      "source": [
        "## Experiment: Naive Iteration"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first define some functions to help with naive iteration.\n",
        "\n",
        "Specifically we define next question extraction function to extract next question based on current passage and then a construct final prompt function to construct a RAG prompt based on 4 given passages"
      ],
      "metadata": {
        "id": "Q3-zdvPPwDBj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def next_q_extraction(ques, passage):\n",
        "\n",
        "  prompt = \"\"\"\n",
        "  Imagine you are answering the question:\n",
        "  {ques}\n",
        "\n",
        "  You have the following imformation in the passage below:\n",
        "\n",
        "  Passage: {passage}\n",
        "\n",
        "  Based on this, what other information do you need to answer this question. Frame it as a concise one-step question.\n",
        "  \"\"\".format(ques = ques, passage = passage)\n",
        "\n",
        "  response = client.completions.create(\n",
        "      model=\"gpt-3.5-turbo-instruct\",\n",
        "      prompt=prompt,\n",
        "      temperature = 0,\n",
        "      max_tokens = 100\n",
        "    )\n",
        "\n",
        "  return response.choices[0].text.strip()\n",
        "\n",
        "\n",
        "def construct_final_prompt(ques, passage_1, passage_2, passage_3, passage_4):\n",
        "\n",
        "  prompt = \"\"\"\n",
        "  You are given the following passages as context:\n",
        "\n",
        "  Passage 1: {passage_1}\n",
        "  Passage 2: {passage_2}\n",
        "  Passage 3: {passage_3}\n",
        "  Passage 4: {passage_4}\n",
        "\n",
        "  Based on information in the above contexts, answer the following question: {ques}\n",
        "  \"\"\".format(ques = ques, passage_1 = passage_1, passage_2 = passage_2, \\\n",
        "             passage_3 = passage_3, passage_4 = passage_4)\n",
        "\n",
        "  return prompt"
      ],
      "metadata": {
        "id": "zI6OBz3Sqa9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZTyYPBbXkxA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b655f66-6718-4798-b3a4-9f8bf8cd803f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [12:38<00:00,  7.59s/it]\n"
          ]
        }
      ],
      "source": [
        "similarity_top_k = 4\n",
        "hit_per_q = []\n",
        "num_hit = 0\n",
        "num_hit_rel = 0\n",
        "responses = []\n",
        "mrr_sum = 0\n",
        "engine = index.as_query_engine(similarity_top_k = similarity_top_k)\n",
        "retrievals = []\n",
        "\n",
        "for i in tqdm(range(100)):\n",
        "  hit_perc = 0\n",
        "  q = questions[i]\n",
        "  hit_flag = False\n",
        "\n",
        "  query_result = engine.query(q)\n",
        "\n",
        "  hits = [query_result.source_nodes[i].text for i in range(len(query_result.source_nodes))]\n",
        "\n",
        "  first_hit = hits[0]\n",
        "\n",
        "  next_q = next_q_extraction(q, first_hit)\n",
        "\n",
        "  query_result_new = engine.query(next_q)\n",
        "\n",
        "  new_hits = [query_result_new.source_nodes[i].text for i in range(len(query_result_new.source_nodes))]\n",
        "\n",
        "  first_new_hit = new_hits[0]\n",
        "\n",
        "  final_prompt = construct_final_prompt(q, passage_1 = first_hit, passage_2 = hits[1], \\\n",
        "                                        passage_3 = new_hits[0], passage_4 = new_hits[1])\n",
        "\n",
        "  retrievals.append([hits[0], hits[1], new_hits[0], new_hits[1]])\n",
        "\n",
        "  response = client.completions.create(\n",
        "      model=\"gpt-3.5-turbo-instruct\",\n",
        "      prompt=final_prompt,\n",
        "      temperature = 0,\n",
        "      max_tokens = 100\n",
        "    )\n",
        "\n",
        "  responses.append(response.choices[0].text.strip())\n",
        "\n",
        "  retrieved_contexts = [hits[0], hits[1], new_hits[0], new_hits[1]]\n",
        "\n",
        "  for j in range(len(supporting_articles[i])):\n",
        "    if supporting_articles[i][j] in retrieved_contexts:\n",
        "      hit_flag = True\n",
        "      num_hit += 1\n",
        "      hit_perc += 1\n",
        "\n",
        "  if hit_flag:\n",
        "    num_hit_rel += 1\n",
        "\n",
        "  hit_per_q += [hit_perc]\n",
        "\n",
        "  for j in range(len(retrieved_contexts)):\n",
        "    if hits[j] in supporting_articles[i]:\n",
        "      mrr_sum += (1/(j+1))\n",
        "      break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sum([hit_per_q[i] == 0 for i in range(100)]))\n",
        "print(sum([hit_per_q[i] == 1 for i in range(100)]))\n",
        "print(sum([hit_per_q[i] == 2 for i in range(100)]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c162111-2860-4715-eedc-54803ff2dfa2",
        "id": "rAtoyKtS-IHv"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n",
            "37\n",
            "58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hard_hits = 0\n",
        "num_hard = 0\n",
        "for i in range(100):\n",
        "  if level[i] == \"medium\":\n",
        "    num_hard += 1\n",
        "    if hit_per_q[i] == 2:\n",
        "      hard_hits += 1\n",
        "\n",
        "\n",
        "\n",
        "print(num_hard)\n",
        "print(hard_hits / num_hard)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2263713b-d8f2-4cd2-b3df-8e061ff4bdff",
        "id": "GOFeGjwa-IHw"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "57\n",
            "0.6491228070175439\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(num_hit_rel / 100)\n",
        "print(num_hit / 200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8104efad-ee43-4e20-9648-7d28aef98550",
        "id": "DA0pBrsh-IHw"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.95\n",
            "0.765\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(mrr_sum)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eecfad6c-02bc-450b-f0f7-0a47c49b436d",
        "id": "ROv7a7IY-IHw"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "92.66666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ans_cor = []\n",
        "#We run this check but manually evaluate the CSV extracted later\n",
        "for i in tqdm(range(len(questions))):\n",
        "  prompt = create_prompt_check(i)\n",
        "\n",
        "\n",
        "\n",
        "  response = client.completions.create(\n",
        "    model=\"gpt-3.5-turbo-instruct\",\n",
        "    prompt=prompt,\n",
        "    temperature = 0\n",
        "  )\n",
        "\n",
        "\n",
        "  if response.choices[0].text.strip().lower().startswith(\"yes\"):\n",
        "    ans_cor.append(1)\n",
        "  else:\n",
        "    ans_cor.append(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e03b0c7-8a89-4c20-cd18-ae2a84e2fb4f",
        "id": "B74p3owBIu9-"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:39<00:00,  2.51it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#accuracy of the answers\n",
        "\n",
        "np.sum(ans_cor) / len(ans_cor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5456bd92-eeb2-4fd7-967a-57de523b87d0",
        "id": "ybarl7W7Iu-D"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.63"
            ]
          },
          "metadata": {},
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Exporting this evaluation as a dataframe\n",
        "\n",
        "eval_df = pd.DataFrame({\n",
        "    'question': questions,\n",
        "    'ground_truth': answers,\n",
        "    'rag_response': responses,\n",
        "    'level': level,\n",
        "    'correct_or_not': ans_cor\n",
        "})\n",
        "\n",
        "eval_df.to_csv(\"vanilla_hotpot_eval_iter.csv\")"
      ],
      "metadata": {
        "id": "6uUTPJ1uIu-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment: Guided Iteration"
      ],
      "metadata": {
        "id": "cWBh3tHBSONt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first create helper functions similar to the other case in order. These would include a function to check if a claim is implied by facts for the conditional in guided retrieval. A create claim function to create a \"claim\" from a question and an answer. We noticed that facts -> claim checking is empiricaly slightly better than question context answer checking.\n",
        "\n",
        "We naturally have a similar next question extraction function and a construct final prompt function"
      ],
      "metadata": {
        "id": "nRfiWnsZA_dd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_reflection_prompt(facts, claim):\n",
        "\n",
        "  refl_prompt = \"\"\"\n",
        "  Based on the given facts, is it reasonable to say that the following claim is true?\n",
        "\n",
        "  Facts: {facts}\n",
        "  Claim: {claim}\n",
        "\n",
        "  Think step by step. End you answer with either yes or no.\n",
        "  \"\"\".format(facts = facts, claim = claim)\n",
        "\n",
        "  return refl_prompt\n",
        "\n",
        "\n",
        "def create_claim(question, answer):\n",
        "\n",
        "  claim_prompt = \"\"\"\n",
        "  Convert the following question and its answer into a simple, one-sentence claim.\n",
        "  Make sure all details from the question are present in the claim.\n",
        "\n",
        "  Question: {question}\n",
        "  Answer: {answer}\n",
        "  \"\"\".format(question = question, answer = answer)\n",
        "\n",
        "  client = OpenAI(api_key = os.environ['OPENAI_API_KEY'])\n",
        "\n",
        "  claim = client.completions.create(\n",
        "    model=\"gpt-3.5-turbo-instruct\",\n",
        "    prompt=claim_prompt,\n",
        "    temperature = 0,\n",
        "    max_tokens = 100\n",
        "  )\n",
        "\n",
        "  return claim.choices[0].text.strip()\n",
        "\n",
        "def next_q_extraction_guided(ques, passage):\n",
        "\n",
        "  prompt = \"\"\"\n",
        "  You are the given the following information as context:\n",
        "\n",
        "  -----------\n",
        "  Context\n",
        "  {passage}\n",
        "  -----------\n",
        "\n",
        "  Based on the information in the context passage below, ask a follow up question\n",
        "  so that you can better answer the question: {ques}.\n",
        "  \"\"\".format(ques = ques, passage = passage)\n",
        "\n",
        "  response = client.completions.create(\n",
        "      model=\"gpt-3.5-turbo-instruct\",\n",
        "      prompt=prompt,\n",
        "      temperature = 0,\n",
        "      max_tokens = 100\n",
        "    )\n",
        "\n",
        "  return response.choices[0].text.strip()\n",
        "\n",
        "\n",
        "def construct_final_prompt_guided(ques, passage_1, passage_2, passage_3, passage_4):\n",
        "\n",
        "  prompt = \"\"\"\n",
        "  You are given the following passages as context\n",
        "\n",
        "  Passage 1: {passage_1}\n",
        "  Passage 2: {passage_2}\n",
        "  Passage 3: {passage_3}\n",
        "  Passage 4: {passage_4}\n",
        "\n",
        "  Based on the information in the context passages below, answer the question: {ques}\n",
        "  \"\"\".format(ques = ques, passage_1 = passage_1, passage_2 = passage_2, \\\n",
        "             passage_3 = passage_3, passage_4 = passage_4)\n",
        "\n",
        "  return prompt\n"
      ],
      "metadata": {
        "id": "RvvrNzJW_F3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluation block\n",
        "client = OpenAI(api_key = os.environ['OPENAI_API_KEY'])\n",
        "similarity_top_k = 4\n",
        "hit_per_q = []\n",
        "num_hit = 0\n",
        "num_hit_rel = 0\n",
        "responses = []\n",
        "mrr_sum = 0\n",
        "engine = index.as_query_engine(similarity_top_k = similarity_top_k,\n",
        "                               llm = oai(temperature = 0, model = \"gpt-3.5-turbo-instruct\"))\n",
        "\n",
        "# retrievals = []\n",
        "\n",
        "for i in tqdm(range(20, 100)):\n",
        "  print(\"Iteration \" + str(i))\n",
        "  hit_perc = 0\n",
        "  q = questions[i]\n",
        "  hit_flag = False\n",
        "\n",
        "  # query_result = engine.query(q)\n",
        "  query_result = engine.query(q)\n",
        "\n",
        "  hits = [query_result.source_nodes[i].text for i in range(len(query_result.source_nodes))]\n",
        "\n",
        "  facts_string = \"\"\n",
        "\n",
        "  for j in range(2):\n",
        "    facts_string += \"Fact \" + str(j+1) + \": \" + hits[j] + \"\\n\"\n",
        "\n",
        "  claim = create_claim(q, query_result.response)\n",
        "  print(f\"Question: {q}\")\n",
        "  print(f\"Response: {query_result.response}\")\n",
        "  print(f\"Claim: {claim}\")\n",
        "\n",
        "  print(f\"Facts string: {facts_string}\")\n",
        "\n",
        "  refl_prompt = create_reflection_prompt(facts_string, query_result.response)\n",
        "\n",
        "  reflection = client.completions.create(\n",
        "    model=\"gpt-3.5-turbo-instruct\",\n",
        "    prompt=refl_prompt,\n",
        "    temperature = 0,\n",
        "    max_tokens = 100\n",
        "  )\n",
        "\n",
        "  print(reflection.choices[0].text.strip())\n",
        "\n",
        "  if reflection.choices[0].text.strip().lower().split(' ')[0].startswith(\"no\"):\n",
        "\n",
        "    print(\"follow-up question chosen\")\n",
        "\n",
        "    next_q = next_q_extraction_guided(q, facts_string)\n",
        "\n",
        "    print(f\"Next question: {next_q}\")\n",
        "\n",
        "    # query_result_new = engine.query(next_q)\n",
        "    query_result_new = engine.query(next_q)\n",
        "\n",
        "    new_hits = [query_result_new.source_nodes[i].text for i in range(len(query_result_new.source_nodes))]\n",
        "\n",
        "    final_prompt = construct_final_prompt_guided(q, passage_1 = hits[0], passage_2 = hits[1], \\\n",
        "                                          passage_3 = new_hits[0], passage_4 = new_hits[1])\n",
        "\n",
        "\n",
        "    retrievals.append([hits[0], hits[1], new_hits[0], new_hits[1]])\n",
        "\n",
        "    response = client.completions.create(\n",
        "      model=\"gpt-3.5-turbo-instruct\",\n",
        "      prompt=final_prompt,\n",
        "      temperature = 0,\n",
        "      max_tokens = 100\n",
        "    ).choices[0].text.strip()\n",
        "\n",
        "    retrieved_contexts = [hits[0], hits[1], new_hits[0], new_hits[1]]\n",
        "\n",
        "\n",
        "  else:\n",
        "\n",
        "    response = query_result.response\n",
        "\n",
        "    retrievals.append(hits)\n",
        "\n",
        "    retrieved_contexts = hits\n",
        "\n",
        "\n",
        "  responses.append(response)\n",
        "\n",
        "\n",
        "  for j in range(len(supporting_articles[i])):\n",
        "    if supporting_articles[i][j] in retrieved_contexts:\n",
        "      hit_flag = True\n",
        "      num_hit += 1\n",
        "      hit_perc += 1\n",
        "\n",
        "  if hit_flag:\n",
        "    num_hit_rel += 1\n",
        "\n",
        "  hit_per_q += [hit_perc]\n",
        "\n",
        "  for j in range(len(retrieved_contexts)):\n",
        "    if hits[j] in supporting_articles[i]:\n",
        "      mrr_sum += (1/(j+1))\n",
        "      break"
      ],
      "metadata": {
        "id": "5wufNiKZ_GGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sum([hit_per_q[i] == 0 for i in range(100)]))\n",
        "print(sum([hit_per_q[i] == 1 for i in range(100)]))\n",
        "print(sum([hit_per_q[i] == 2 for i in range(100)]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65190f6d-4054-44b2-cc66-8be3e4127f3f",
        "id": "nmcFImNGNBhB"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n",
            "43\n",
            "53\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hard_hits = 0\n",
        "num_hard = 0\n",
        "for i in range(len(responses)):\n",
        "  if level[i] == \"easy\":\n",
        "    num_hard += 1\n",
        "    if hit_per_q[i] == 2:\n",
        "      hard_hits += 1\n",
        "\n",
        "\n",
        "\n",
        "print(num_hard)\n",
        "print(hard_hits / num_hard)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "710c10da-342a-4b54-b77c-88f6139f175b",
        "id": "uRhG6zHDNBhC"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "27\n",
            "0.2962962962962963\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(num_hit_rel / 100)\n",
        "print(num_hit / 200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "476e160d-d80a-48ec-9e52-9f63bf8b0cb0",
        "id": "bnOQj_BONBhC"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.96\n",
            "0.745\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ans_cor = []\n",
        "\n",
        "#We run the v1 check and then manually evaluate later\n",
        "for i in tqdm(range(len(responses))):\n",
        "  prompt = create_prompt_check(i)\n",
        "\n",
        "\n",
        "\n",
        "  response = client.completions.create(\n",
        "    model=\"gpt-3.5-turbo-instruct\",\n",
        "    prompt=prompt,\n",
        "    temperature = 0\n",
        "  )\n",
        "\n",
        "\n",
        "  if response.choices[0].text.strip().lower().startswith(\"yes\"):\n",
        "    ans_cor.append(1)\n",
        "  else:\n",
        "    ans_cor.append(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96379517-ad5d-4602-b151-236b3e134054",
        "id": "TYloT_d2NBhC"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:36<00:00,  2.72it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#accuracy of the answers\n",
        "\n",
        "np.sum(ans_cor) / len(ans_cor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27084731-f1bd-44b9-9015-37a222672c65",
        "id": "BZpeVo_2NBhC"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.68"
            ]
          },
          "metadata": {},
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Exporting this evaluation as a dataframe\n",
        "\n",
        "eval_df = pd.DataFrame({\n",
        "    'question': questions,\n",
        "    'ground_truth': answers,\n",
        "    'rag_response': responses,\n",
        "    'level': level,\n",
        "    'correct_or_not': ans_cor\n",
        "})\n",
        "\n",
        "eval_df.to_csv(\"vanilla_hotpot_eval_refl_2.csv\")"
      ],
      "metadata": {
        "id": "r5P5Xt8bNBhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tree-RAG\n",
        "\n",
        "After the failure of a decomposition based strategy, we will instead try and see if a tree based strategy works instead.\n",
        "\n",
        "We once again create helper functions to implement this"
      ],
      "metadata": {
        "id": "rDtuVvQ-BuOd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def next_q_extraction_tree(ques, passage):\n",
        "\n",
        "  prompt = \"\"\"\n",
        "  I know the following information in context:\n",
        "\n",
        "  -----------\n",
        "  Context\n",
        "  {passage}\n",
        "  -----------\n",
        "\n",
        "  I need to answer the question: {ques}.\n",
        "  What more information do I need in addition to the context to answer this?\n",
        "  Frame your response as a simple, one hop follow-up question.\n",
        "  \"\"\".format(ques = ques, passage = passage)\n",
        "\n",
        "  response = client.completions.create(\n",
        "      model=\"gpt-3.5-turbo-instruct\",\n",
        "      prompt=prompt,\n",
        "      temperature = 0,\n",
        "      max_tokens = 100\n",
        "    )\n",
        "\n",
        "  return response.choices[0].text.strip()\n",
        "\n",
        "\n",
        "def construct_final_prompt_tree(ques, passage_1, passage_2, passage_3):\n",
        "\n",
        "  prompt = \"\"\"\n",
        "  You are given the following passages as context\n",
        "\n",
        "  Passage 1: {passage_1}\n",
        "  Passage 2: {passage_2}\n",
        "  Passage 3: {passage_3}\n",
        "\n",
        "  Based on the information in the context passages below, answer the following question:\n",
        "\n",
        "  Question: {ques}\n",
        "  \"\"\".format(ques = ques, passage_1 = passage_1, passage_2 = passage_2, \\\n",
        "             passage_3 = passage_3)\n",
        "\n",
        "  return prompt\n"
      ],
      "metadata": {
        "id": "gEM0bT1nRNDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluation block\n",
        "client = OpenAI(api_key = os.environ['OPENAI_API_KEY'])\n",
        "similarity_top_k = 4\n",
        "hit_per_q = []\n",
        "num_hit = 0\n",
        "num_hit_rel = 0\n",
        "responses = []\n",
        "mrr_sum = 0\n",
        "engine = index.as_query_engine(similarity_top_k = similarity_top_k,\n",
        "                               llm = oai(temperature = 0, model = \"gpt-3.5-turbo-instruct\"))\n",
        "\n",
        "retrievals = []\n",
        "\n",
        "for i in tqdm(range(0, 100)):\n",
        "  retrieved_contexts = []\n",
        "  print(\"Iteration \" + str(i))\n",
        "  hit_perc = 0\n",
        "  q = questions[i]\n",
        "  hit_flag = False\n",
        "\n",
        "  # query_result = engine.query(q)\n",
        "  query_result = engine.query(q)\n",
        "\n",
        "  hits = [query_result.source_nodes[i].text for i in range(len(query_result.source_nodes))]\n",
        "\n",
        "\n",
        "  print(\"follow-up question chosen\")\n",
        "\n",
        "  new_contexts = []\n",
        "  next_questions = []\n",
        "\n",
        "  for j in range(len(hits[:3])):\n",
        "    next_qs = next_q_extraction_tree(q, hits[j])\n",
        "    query_result_new = engine.query(next_qs)\n",
        "    new_con = [hits[j]] + [query_result_new.source_nodes[k].text for k in range(2)]\n",
        "    retrieved_contexts.extend(new_con)\n",
        "    new_contexts.append('. '.join(new_con))\n",
        "    next_questions.append(next_qs)\n",
        "\n",
        "\n",
        "  final_prompt = construct_final_prompt_tree(q, passage_1 = new_contexts[0], passage_2 = new_contexts[1], \\\n",
        "                                        passage_3 = new_contexts[2])\n",
        "\n",
        "  print(final_prompt)\n",
        "  response = client.completions.create(\n",
        "      model=\"gpt-3.5-turbo-instruct\",\n",
        "      prompt=final_prompt,\n",
        "      temperature = 0,\n",
        "      max_tokens = 100,\n",
        "      logprobs = 1\n",
        "    ).choices[0].text.strip()\n",
        "\n",
        "\n",
        "  responses.append(response)\n",
        "\n",
        "\n",
        "  for j in range(len(supporting_articles[i])):\n",
        "    if supporting_articles[i][j] in retrieved_contexts:\n",
        "      hit_flag = True\n",
        "      num_hit += 1\n",
        "      hit_perc += 1\n",
        "\n",
        "  if hit_flag:\n",
        "    num_hit_rel += 1\n",
        "\n",
        "  hit_per_q += [hit_perc]"
      ],
      "metadata": {
        "id": "gofMs3EoRNDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hard_hits = 0\n",
        "num_hard = 0\n",
        "for i in range(len(responses)):\n",
        "  if level[i] == \"hard\":\n",
        "    num_hard += 1\n",
        "    if hit_per_q[i] == 2:\n",
        "      hard_hits += 1\n",
        "\n",
        "\n",
        "\n",
        "print(num_hard)\n",
        "print(hard_hits / num_hard)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8a162f0-42fb-4637-b36f-ccd04419a193",
        "id": "lzV8-XRfQ4gD"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16\n",
            "0.6875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hard_hits = 0\n",
        "for i in range(len(responses)):\n",
        "  if hit_per_q[i] == 2:\n",
        "    hard_hits += 1\n",
        "\n",
        "hard_hits/100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7779bcf9-7c1c-4aa7-ace1-759c286f91eb",
        "id": "4Kg-GMupQ4gL"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.69"
            ]
          },
          "metadata": {},
          "execution_count": 275
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(num_hit_rel / 100)\n",
        "print(num_hit / 200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c393257c-9a6f-4601-93b2-fd5e5aa4cd2a",
        "id": "EPR-AdpVQ4gL"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.98\n",
            "0.835\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ans_cor = []\n",
        "\n",
        "#We run this as a v1 check and then manually evaluate later\n",
        "for i in tqdm(range(len(responses))):\n",
        "  prompt = create_prompt_check(i)\n",
        "\n",
        "\n",
        "\n",
        "  response = client.completions.create(\n",
        "    model=\"gpt-3.5-turbo-instruct\",\n",
        "    prompt=prompt,\n",
        "    temperature = 0\n",
        "  )\n",
        "\n",
        "\n",
        "  if response.choices[0].text.strip().lower().startswith(\"yes\"):\n",
        "    ans_cor.append(1)\n",
        "  else:\n",
        "    ans_cor.append(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96ac40e2-2822-46cb-c894-64f675fe3bd1",
        "id": "NLCmkiOIQ4gL"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:37<00:00,  2.65it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#accuracy of the answers\n",
        "\n",
        "np.sum(ans_cor) / len(ans_cor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be2a47bf-2031-422a-d395-865d6d19811e",
        "id": "IUNlkz3XQ4gL"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.74"
            ]
          },
          "metadata": {},
          "execution_count": 273
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Exporting this evaluation as a dataframe\n",
        "\n",
        "eval_df = pd.DataFrame({\n",
        "    'question': questions,\n",
        "    'ground_truth': answers,\n",
        "    'rag_response': responses,\n",
        "    'level': level,\n",
        "    'correct_or_not': ans_cor\n",
        "})\n",
        "\n",
        "eval_df.to_csv(\"vanilla_hotpot_eval_tree.csv\")"
      ],
      "metadata": {
        "id": "4MQrVhuiQ4gL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bd8acfc00a364309819f1b9ee383f5eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c749d8363dd543cdb054774ee99d86bc",
              "IPY_MODEL_6e03d1c36df54b1696d2acf93506bb27",
              "IPY_MODEL_186f4b826e0f41e79d5662725e4598d9"
            ],
            "layout": "IPY_MODEL_c54731489f0944ff90e68ace218b949f"
          }
        },
        "c749d8363dd543cdb054774ee99d86bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a40181f93e9a43b99b884ee37a64964e",
            "placeholder": "​",
            "style": "IPY_MODEL_1ecdbc8a49a443beac00462cc951d8a4",
            "value": "Downloading data: 100%"
          }
        },
        "6e03d1c36df54b1696d2acf93506bb27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d12032a287c4e04bbce3eda6a4071f6",
            "max": 300793940,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_56d364085db14ea2a30d3cedb02d19d3",
            "value": 300793940
          }
        },
        "186f4b826e0f41e79d5662725e4598d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f53cbd2428f40678cc9f7416d82b29d",
            "placeholder": "​",
            "style": "IPY_MODEL_3f8e69f8fbe24fa0bab6999f8b3ed18e",
            "value": " 301M/301M [00:06&lt;00:00, 52.4MB/s]"
          }
        },
        "c54731489f0944ff90e68ace218b949f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a40181f93e9a43b99b884ee37a64964e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ecdbc8a49a443beac00462cc951d8a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7d12032a287c4e04bbce3eda6a4071f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56d364085db14ea2a30d3cedb02d19d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6f53cbd2428f40678cc9f7416d82b29d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f8e69f8fbe24fa0bab6999f8b3ed18e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d5aa3083f08945fdb20e5d149daa0e06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e6d5b207a9724c70bdbb81ac9f7d33a7",
              "IPY_MODEL_7e43f482a533400c86711f2e789add89",
              "IPY_MODEL_d69ee5c57f4d4d51b8b674b436849d5d"
            ],
            "layout": "IPY_MODEL_08a30059c3734c33aa0b53f63f02bf39"
          }
        },
        "e6d5b207a9724c70bdbb81ac9f7d33a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f509c7c745204db1a9d46177311ce67e",
            "placeholder": "​",
            "style": "IPY_MODEL_de2f096f6e8c48609cf8caef3752f7b7",
            "value": "Downloading data: 100%"
          }
        },
        "7e43f482a533400c86711f2e789add89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb917b0891774050b8c788ec08a24712",
            "max": 31103583,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_605e1d3c5c1a44efb2221f2ed609f5dc",
            "value": 31103583
          }
        },
        "d69ee5c57f4d4d51b8b674b436849d5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d581930dbe1844cba69989ef3eaebf0d",
            "placeholder": "​",
            "style": "IPY_MODEL_0cc8105429f948c3ba0695a6264c26d5",
            "value": " 31.1M/31.1M [00:00&lt;00:00, 47.4MB/s]"
          }
        },
        "08a30059c3734c33aa0b53f63f02bf39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f509c7c745204db1a9d46177311ce67e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de2f096f6e8c48609cf8caef3752f7b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fb917b0891774050b8c788ec08a24712": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "605e1d3c5c1a44efb2221f2ed609f5dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d581930dbe1844cba69989ef3eaebf0d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0cc8105429f948c3ba0695a6264c26d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9c2ea52a42f94179b51ab3e75324faac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f1e2db5305a64ca2a91397670adcc7d9",
              "IPY_MODEL_0b0cd9d0db604907a739690bf41aeede",
              "IPY_MODEL_60203443bc3d4eeca6267f91143235b9"
            ],
            "layout": "IPY_MODEL_c9a25e6d20f940e29b5dc2fed9452527"
          }
        },
        "f1e2db5305a64ca2a91397670adcc7d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_09b17a3214f1407b81543e63576af05e",
            "placeholder": "​",
            "style": "IPY_MODEL_e70a6d7cbce343fe8399ea5da51bf5c2",
            "value": "Downloading data: 100%"
          }
        },
        "0b0cd9d0db604907a739690bf41aeede": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_92b8ad64e3cf415c9d509bbcf611e6e3",
            "max": 27462047,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_328c637233c545a1bbe7e30c4e77627d",
            "value": 27462047
          }
        },
        "60203443bc3d4eeca6267f91143235b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_52944e9d3d2e4bd3bab02f87c9572fad",
            "placeholder": "​",
            "style": "IPY_MODEL_a8151b84286d4614b064745aaf6e6e63",
            "value": " 27.5M/27.5M [00:00&lt;00:00, 42.1MB/s]"
          }
        },
        "c9a25e6d20f940e29b5dc2fed9452527": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09b17a3214f1407b81543e63576af05e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e70a6d7cbce343fe8399ea5da51bf5c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "92b8ad64e3cf415c9d509bbcf611e6e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "328c637233c545a1bbe7e30c4e77627d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "52944e9d3d2e4bd3bab02f87c9572fad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8151b84286d4614b064745aaf6e6e63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ca8b22e2ca88404492daa9c6d9b79b36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9ca8fdc025ef49dc828b84ee2562ec81",
              "IPY_MODEL_c97c6a7b8a164eec97ad565b8e681728",
              "IPY_MODEL_1150ac74e6d944eaa5e42858271e066d"
            ],
            "layout": "IPY_MODEL_881c8117ef81444ba53d7194f7ddd047"
          }
        },
        "9ca8fdc025ef49dc828b84ee2562ec81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb63fbf9a71348b08682b6d37d3e89a9",
            "placeholder": "​",
            "style": "IPY_MODEL_7417376e2a954792a045d9447eb87799",
            "value": "Generating train split: 100%"
          }
        },
        "c97c6a7b8a164eec97ad565b8e681728": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea96c41e05bb487091e4984454e48989",
            "max": 90447,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_124b9d76a46440dbb134cc70ba560e0d",
            "value": 90447
          }
        },
        "1150ac74e6d944eaa5e42858271e066d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76d1baa0125947349d04ac0cb3024163",
            "placeholder": "​",
            "style": "IPY_MODEL_8a7ccd159d6c482584e9386b17d3e58a",
            "value": " 90447/90447 [00:02&lt;00:00, 37121.18 examples/s]"
          }
        },
        "881c8117ef81444ba53d7194f7ddd047": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb63fbf9a71348b08682b6d37d3e89a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7417376e2a954792a045d9447eb87799": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ea96c41e05bb487091e4984454e48989": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "124b9d76a46440dbb134cc70ba560e0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "76d1baa0125947349d04ac0cb3024163": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a7ccd159d6c482584e9386b17d3e58a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3783e00fb4084d0ebabf53a6fc34714e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3a7852d5421b4a2184e06c6d30f64d57",
              "IPY_MODEL_08cdcd40f3f54277833899c3cf2ecaee",
              "IPY_MODEL_2d43293bd0904fdaa606ca9e3b81e391"
            ],
            "layout": "IPY_MODEL_b522c2adf61a4e318665fe2c25117d58"
          }
        },
        "3a7852d5421b4a2184e06c6d30f64d57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8af285305f84d3d8f04fb7324f2ee43",
            "placeholder": "​",
            "style": "IPY_MODEL_e8ee252696b1408f83dc78e2218b2674",
            "value": "Generating validation split: 100%"
          }
        },
        "08cdcd40f3f54277833899c3cf2ecaee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0fc736e1f6744eb5a818628a2e2e6481",
            "max": 7405,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4183eb71ab05490ea949f7f350def58e",
            "value": 7405
          }
        },
        "2d43293bd0904fdaa606ca9e3b81e391": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23307b0a6e1e42d0b55bc9bd066d2f24",
            "placeholder": "​",
            "style": "IPY_MODEL_99ca4bc670b84bc5b4ec952ce2c3eae9",
            "value": " 7405/7405 [00:00&lt;00:00, 33144.96 examples/s]"
          }
        },
        "b522c2adf61a4e318665fe2c25117d58": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8af285305f84d3d8f04fb7324f2ee43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8ee252696b1408f83dc78e2218b2674": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0fc736e1f6744eb5a818628a2e2e6481": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4183eb71ab05490ea949f7f350def58e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "23307b0a6e1e42d0b55bc9bd066d2f24": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99ca4bc670b84bc5b4ec952ce2c3eae9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}